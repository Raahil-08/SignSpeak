# ğŸ§  SignSpeak: Real-Time ASL Recognition System

![Banner](./SignSpeak.png)

[![Release](https://img.shields.io/github/v/release/CodeWithInferno/SignSpeak)](https://github.com/CodeWithInferno/SignSpeak/releases)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)

---

## ğŸš€ Overview

SignSpeak is a cross-platform American Sign Language (ASL) recognition platform that:

* ğŸ•’ **Detects ASL signs in real-time** via your device camera
* ğŸ“± **Runs on mobile devices** (iOS & Android) using React Native + Expo
* ğŸ¤– **Processes frames** on a Python Flask backend with MediaPipe, OpenCV, and trained ML models
* ğŸ—ï¸ **Dockerized** for oneâ€‘command deployment (development or production)

> ğŸ¥ **Model Demo**: [Watch proof video](./Model_proof.mp4)

---

## ğŸ“‹ Table of Contents

- [ğŸ§  SignSpeak: Real-Time ASL Recognition System](#-signspeak-real-time-asl-recognition-system)
  - [ğŸš€ Overview](#-overview)
  - [ğŸ“‹ Table of Contents](#-table-of-contents)
  - [ğŸ”¥ Features](#-features)
  - [ğŸ—‚ Project Structure](#-project-structure)
  - [âš™ï¸ Getting Started](#ï¸-getting-started)
    - [Prerequisites](#prerequisites)
    - [Local Development](#local-development)
    - [ğŸ³ Docker Deployment](#-docker-deployment)
  - [ğŸ–¼ï¸ Assets](#ï¸-assets)
  - [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
  - [ğŸ¤ Contributing](#-contributing)
  - [ğŸ“– License](#-license)
  - [ğŸ“ Contact](#-contact)

---

## ğŸ”¥ Features

* **Real-Time Recognition**: 30+ frames per second hand detection with MediaPipe
* **Multiple Strategies**:

  * Keypoint-based classification
  * CNN-powered image classification
  * Optional YOLOv5 object detection pipeline
* **Session Recording**: Start/stop sessions, save raw frames, export metadata
* **Cross-Platform UI**: Shared codebase for iOS, Android, and Web via Expo
* **Scalable Backend**: Containerized Flask API, ready for production with Gunicorn + Nginx

---

## ğŸ—‚ Project Structure

```
SignSpeak/
â”œâ”€â”€ UI_expo/           # React Native (Expo) mobile app
â”‚   â”œâ”€â”€ components/    # UI & camera overlay
â”‚   â”œâ”€â”€ services/      # API wrappers & data handling
â”‚   â””â”€â”€ app.json       # Expo config
â”œâ”€â”€ server/            # Flask backend API
â”‚   â”œâ”€â”€ App.py         # Main application
â”‚   â”œâ”€â”€ recordings/    # Stored session data
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ models/            # Pretrained ML models & label files
â”‚   â”œâ”€â”€ random_forest_model.pkl
â”‚   â””â”€â”€ label_classes.txt
â”œâ”€â”€ deploy/            # Docker & Nginx configs
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ SignSpeak.png       # README banner
â”‚   â”œâ”€â”€ ASL Alphabet.jpg    # ASL chart reference
â”‚   â””â”€â”€ Model_proof.mp4     # Demo video
â”œâ”€â”€ docs/              # (Optional) Extended documentation
â”œâ”€â”€ Scripts/           # Training & preprocessing scripts
â””â”€â”€ README.md          # This file
```

---

## âš™ï¸ Getting Started

### Prerequisites

* [Node.js & npm](https://nodejs.org/) (v14+)
* [Python 3.8+](https://www.python.org/)
* [Docker & Docker Compose](https://docs.docker.com/)

### Local Development

1. **Clone the repository**

   ```bash
   git clone https://github.com/CodeWithInferno/SignSpeak.git
   cd SignSpeak
   ```

2. **Backend**

   ```bash
   cd server
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   python App.py
   ```

   The API will run at: `http://localhost:5050`

3. **Mobile App**

   ```bash
   cd UI_expo
   npm install
   npx expo start
   ```

   * Open the Expo Go app on your phone and scan the QR code, or launch on an emulator.

---

### ğŸ³ Docker Deployment

1. **Start with Docker Compose**

   ```bash
   docker-compose up --build -d
   ```
2. **Verify services**

   ```bash
   docker-compose ps
   ```
3. **Access UI** on your LAN at `http://<your-host-ip>:19006`

---

## ğŸ–¼ï¸ Assets

* **Banner**: `./SignSpeak.png`

* **ASL Alphabet Chart**:

  ![ASL Alphabet](./ASL%20Alphabet.jpg)

* **Model Proof Video**:

  <video width="400" controls>
    <source src="./Model_proof.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>

---

## ğŸ› ï¸ Tech Stack

| Layer      | Technology                                         |
| ---------- | -------------------------------------------------- |
| Frontend   | React Native (Expo)                                |
| Backend    | Python, Flask, Flask-CORS                          |
| CV/ML      | MediaPipe Hands, OpenCV, scikit-learn (RF), YOLOv5 |
| Deployment | Docker, Docker Compose, Gunicorn                   |
| Platform   | Android, iOS & Web (Progressive)                   |

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/YourFeature`)
3. Commit your changes (`git commit -m "Add YourFeature"`)
4. Push (`git push origin feature/YourFeature`)
5. Open a Pull Request

For major changes, open an issue first to discuss what youâ€™d like to change.

---

## ğŸ“– License

This project is licensed under the MIT License. See [LICENSE](./LICENSE) for details.

---

## ğŸ“ Contact

* GitHub Issues: [codewithinferno/signspeak](https://github.com/CodeWithInferno/SignSpeak/issues)
* Discussions: [GitHub Discussions](https://github.com/CodeWithInferno/SignSpeak/discussions)
* Email: [praathambiren2618@gmail.com](mailto:prathambiren2618@gmail.com)

---


> Made with â¤ï¸ by Pratham Patel, Raahil Desai & Tashvi Patel

